{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation\n",
    "\n",
    "We have already mentioned the computer taking a derivative. For example, if you do not supply a gradient to an optimization routine like Quasi-Newton BFGS, the routine will still run. How?\n",
    "\n",
    "### Option 1: Take a derivative by hand and write a function\n",
    "\n",
    "This is self explanatory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Option2:  Finite Differences\n",
    "\n",
    "$$ f'(x) \\approx  \\frac{f(x+h) - f(x)}{h} $$\n",
    "\n",
    "where $h=\\max\\{ \\epsilon \\times | x | , \\epsilon\\}$ and $\\epsilon$ is small, like $10^{-6}$.\n",
    "\n",
    "We want h to be small compared to x, but numerical it cannot be too small or else we divide by something close to zero. \n",
    "\n",
    "**Multivariate Version**\n",
    "\n",
    "$$ f_i(x_1,...,x_n) \\approx  \\frac{f(x_i+h_i;\\mathbf{x}) - f(\\mathbf{x})}{h_i}    $$\n",
    "\n",
    "But the number of evaluations of $f(x)$ is $n+1$. So finite differences can add substantial computation time, compared to explicit derivatives. Plus, they are less accurate. \n",
    "\n",
    "**2-sided Finite Differences**\n",
    "\n",
    "$$ f'(x) \\approx  \\frac{f(x+h) - f(x-h)}{2h}   $$ \n",
    "\n",
    "which will reduce errors but now costs twice the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = x.^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finDiff (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function finDiff(g,x,hh=10e-6)\n",
    "    h = max(hh*x,hh)\n",
    "    gg = (g(x+h) - g(x))./h\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDiff(f,1.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDiff(f,1.0,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.100000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDiff(f,1.0,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.00001000001393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finDiff(f,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Automatic Differentiation\n",
    "\n",
    "The computer is evaluating $f(x)$ by evaluating the individual \"pieces.\" The logic of AD is that we can use the \"sub-evaluations\" the computer is already doing by expressing any derivative in terms of the chain rule. \n",
    "\n",
    "**Example**\n",
    "                $$f(x,y,z) = (x^α + y^α + z^α)^{\\gamma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $\\nabla f$...\n",
    "\n",
    "1. Compute original function, the computer must compute $x^α$, $y^α$, $z^α$, $x^α + y^α + z^α$, and $(x^α + y^α + z^α)^{\\gamma}$;\n",
    "2. Store these values to use later;\n",
    "3. $f_x = (x^α + y^α + z^α)^{\\gamma - 1}\\gamma\\alpha x^{\\alpha - 1}$, so we need 2 divisions and 3 multiplications using the store values...\n",
    "$$ f_x = \\frac{(x^α + y^α + z^α)^{\\gamma}}{x^α + y^α + z^α}\\gamma \\alpha \\frac{x^{\\alpha}}{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational burden is much much lower than finite differences (arithmatic) and the result is the exact derivative.\n",
    "\n",
    "Computer scientists have been developing robust routines to take adavantage of these internal computations and the chain rule. ```Julia``` has packages to compute derivatives this way although I am not aware of a ```Matlab``` package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration\n",
    "\n",
    "In many cases we want the computer to compute a (in)definite integral of $f$ over interval $I$ on $R^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
